{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "67e8fd4b-e795-4385-935f-d3db055e71ac",
    "_uuid": "b8d1c1593b4892a11e30c5362c7557657492c205"
   },
   "source": [
    "# Overview\n",
    "The notebook aims to organize the data and hack Keras so that we can train a model in a fairly simple way. The aim here is to get a model working that can reliably segment the images into objects and then we can make a model that handles grouping the objects into categories based on the labels. As you will see the Keras requires a fair bit of hackery to get it to load images from a dataframe and then get it to read the label images correctly (uint16 isn't supported well). Once that is done, training a U-Net model is really easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from skimage.segmentation import mark_boundaries\n",
    "DATA_DIR = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"../recognizer/data/imgs/input\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_paths = pd.DataFrame(dict(path = glob(os.path.join(DATA_DIR, '*', '*.*p*g'))))\n",
    "all_paths['split'] = all_paths['path'].map(lambda x: x.split('/')[-2].split('_')[0])\n",
    "all_paths['group'] = all_paths['path'].map(lambda x: x.split('/')[-2].split('_')[-1])\n",
    "all_paths['group'] = all_paths['group'].map(lambda x: 'color' if x == 'test' else x)\n",
    "all_paths['id'] = all_paths['path'].map(lambda x: '_'.join(os.path.splitext(os.path.basename(x))[0].split('_')[:4]))\n",
    "all_paths.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "cb0565b2-ff44-4e25-b401-8794472552d7",
    "_uuid": "6892e98b6dcec247664d1b7a521ec25beeb84619"
   },
   "outputs": [],
   "source": [
    "group_df = all_paths.pivot_table(values = 'path', columns = 'group', aggfunc = 'first', index = ['id', 'split']).reset_index()\n",
    "group_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "567d4f91-cb7d-4b04-8798-2d1e1a3fcc14",
    "_uuid": "7019801430e6b9ebe234e07544d2d4ce47d31015"
   },
   "source": [
    "# Explore the training set\n",
    "Here we can show the training data image by image to see what exactly we are supposed to detect with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "2a63d977-02c8-43a9-8f8c-02b3499309c4",
    "_uuid": "d5c261be25e697941928a396e4d6f5a4b1ca4395"
   },
   "outputs": [],
   "source": [
    "train_df = group_df.query('split==\"train\"')\n",
    "print(train_df.shape[0], 'rows')\n",
    "sample_rows = 6\n",
    "fig, m_axs = plt.subplots(sample_rows, 3, figsize = (20, 6*sample_rows))\n",
    "[c_ax.axis('off') for c_ax in m_axs.flatten()]\n",
    "for (ax1, ax2, ax3), (_, c_row) in zip(m_axs, train_df.sample(sample_rows).iterrows()):\n",
    "    c_img = imread(c_row['color'])\n",
    "    l_img = imread(c_row['label'])\n",
    "    if l_img.ndim==3: l_img = l_img[:,:,0]\n",
    "    ax1.imshow(c_img)\n",
    "    ax1.set_title('Color')\n",
    "    # make the labels nicer\n",
    "    nice_limg = np.zeros(l_img.shape, dtype = np.uint8)\n",
    "    for new_idx, old_idx in enumerate(np.unique(l_img[l_img>MIN_OBJ_VAL]), 1):\n",
    "        nice_limg[l_img==old_idx]=new_idx\n",
    "    ax2.imshow(nice_limg, cmap = 'nipy_spectral')\n",
    "    ax2.set_title('Labels')\n",
    "    xd, yd = np.where(l_img>MIN_OBJ_VAL)\n",
    "    bound_img = mark_boundaries(image = c_img, label_img = l_img, color = (1,0,0), background_label = 255, mode = 'thick')\n",
    "    ax3.imshow(bound_img[xd.min():xd.max(), yd.min():yd.max(),:])\n",
    "    ax3.set_title('Cropped Overlay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "41ccd780-75e2-4858-8dd7-26ab54c0290e",
    "_uuid": "c13081e6618b920cdc36285fde271983d2c3a218",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_split_df, valid_split_df = train_test_split(train_df, random_state = 2018, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "ddb1c113-1c53-4bc3-948e-e9d1edadc082",
    "_uuid": "2c17e0ab0d9347cdf6af56b97aae9047be40d766",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "IMG_SIZE = (384, 384) # slightly smaller than vgg16 normally expects\n",
    "img_gen_args = dict(samplewise_center=False, \n",
    "                              samplewise_std_normalization=False, \n",
    "                              horizontal_flip = True, \n",
    "                              vertical_flip = False, \n",
    "                              height_shift_range = 0.1, \n",
    "                              width_shift_range = 0.1, \n",
    "                              rotation_range = 3, \n",
    "                              shear_range = 0.01,\n",
    "                              fill_mode = 'nearest',\n",
    "                              zoom_range = 0.05)\n",
    "\n",
    "rgb_gen = ImageDataGenerator(preprocessing_function = preprocess_input, **img_gen_args)\n",
    "lab_gen = ImageDataGenerator(**img_gen_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "4b67ae84-2f58-425f-9296-dcc7d2d62c9e",
    "_uuid": "0ce7b0154f7d68add10776c10e9053b83b3e255e",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def flow_from_dataframe(img_data_gen, in_df, path_col, y_col, seed = None, **dflow_args):\n",
    "    base_dir = os.path.dirname(in_df[path_col].values[0])\n",
    "    print('## Ignore next message from keras, values are replaced anyways: seed: {}'.format(seed))\n",
    "    df_gen = img_data_gen.flow_from_directory(base_dir, \n",
    "                                     class_mode = 'sparse',\n",
    "                                              seed = seed,\n",
    "                                    **dflow_args)\n",
    "    df_gen.filenames = in_df[path_col].values\n",
    "    df_gen.classes = np.stack(in_df[y_col].values)\n",
    "    df_gen.samples = in_df.shape[0]\n",
    "    df_gen.n = in_df.shape[0]\n",
    "    df_gen._set_index_array()\n",
    "    df_gen.directory = '' # since we have the full path\n",
    "    print('Reinserting dataframe: {} images'.format(in_df.shape[0]))\n",
    "    return df_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "28767464-3a4c-4110-a5a2-f0ecbbcf8f51",
    "_uuid": "f8d2dfeb980142e54ba517815984bb5570ef91a2"
   },
   "source": [
    "## Replace PIL with scikit-image \n",
    "This lets us handle the 16bit numbers well in the instanceIds image. This is incredibly, incredibly hacky, please do not use this code outside of this kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "5951c0bc-1df1-4a8a-8e87-c076d88f7e12",
    "_uuid": "5b435e6727af094fb76eb157a67950178fc54485"
   },
   "outputs": [],
   "source": [
    "import keras.preprocessing.image as KPImage\n",
    "from PIL import Image\n",
    "class pil_image_awesome():\n",
    "    @staticmethod\n",
    "    def open(in_path):\n",
    "        if 'instanceIds' in in_path:\n",
    "            # we only want to keep the positive labels not the background\n",
    "            in_img = imread(in_path)\n",
    "            if in_img.ndim==3:\n",
    "                in_img = in_img[:,:,0]\n",
    "            return Image.fromarray((in_img>MIN_OBJ_VAL).astype(np.float32))\n",
    "        else:\n",
    "            return Image.open(in_path)\n",
    "    fromarray = Image.fromarray\n",
    "KPImage.pil_image = pil_image_awesome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0e9ec0f2-fe4b-4a6f-a9a3-118b4b4f8112",
    "_uuid": "75d08811cbb12786d7cdb8f82ac3b8aa95867e41"
   },
   "source": [
    "# Create the generators\n",
    "We want to generate parallel streams of images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "6c287bc5-9ec0-4291-8747-4d0d80b51dd5",
    "_uuid": "bb9b79c1b61f3f7948e6fb6c50ae859df9f10c3b",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from skimage.filters.rank import maximum\n",
    "from scipy.ndimage import zoom\n",
    "def lab_read_func(in_path):\n",
    "    bin_img = (imread(in_path)>1000).astype(np.uint8)\n",
    "    x_dim, y_dim = bin_img.shape\n",
    "    max_label_img = maximum(bin_img, np.ones((x_dim//IMG_SIZE[0], y_dim//IMG_SIZE[1])))\n",
    "    return np.expand_dims(zoom(max_label_img, (IMG_SIZE[0]/x_dim, IMG_SIZE[1]/y_dim), order = 3), -1)\n",
    "\n",
    "\n",
    "def train_and_lab_gen_func(in_df, batch_size = 8, seed = None):\n",
    "    if seed is None:\n",
    "        seed = np.random.choice(range(1000))\n",
    "    train_rgb_gen = flow_from_dataframe(rgb_gen, in_df, \n",
    "                             path_col = 'color',\n",
    "                            y_col = 'id', \n",
    "                            target_size = IMG_SIZE,\n",
    "                             color_mode = 'rgb',\n",
    "                            batch_size = batch_size,\n",
    "                                   seed = seed)\n",
    "    train_lab_gen = flow_from_dataframe(lab_gen, in_df, \n",
    "                             path_col = 'label',\n",
    "                            y_col = 'id', \n",
    "                            target_size = IMG_SIZE,\n",
    "                             color_mode = 'grayscale',\n",
    "                            batch_size = batch_size,\n",
    "                                   seed = seed)\n",
    "    for (x, _), (y, _) in zip(train_rgb_gen, train_lab_gen):\n",
    "        yield x, y\n",
    "    \n",
    "train_and_lab_gen = train_and_lab_gen_func(train_split_df, batch_size = 32)\n",
    "valid_and_lab_gen = train_and_lab_gen_func(valid_split_df, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "7d2e62b7-de3e-4fd6-9fe7-bde1e28f7c19",
    "_uuid": "abbef517deaa6fed4d2988e6306dc0e388a19851"
   },
   "outputs": [],
   "source": [
    "(rgb_batch, lab_batch) = next(valid_and_lab_gen)\n",
    "\n",
    "sample_rows = 4\n",
    "fig, m_axs = plt.subplots(sample_rows, 3, figsize = (20, 6*sample_rows))\n",
    "[c_ax.axis('off') for c_ax in m_axs.flatten()]\n",
    "for (ax1, ax2, ax3), rgb_img, lab_img in zip(m_axs, rgb_batch, lab_batch):\n",
    "    # undoing the vgg correction is tedious\n",
    "    r_rgb_img = np.clip(rgb_img+110, 0, 255).astype(np.uint8)\n",
    "    ax1.imshow(r_rgb_img)\n",
    "    ax1.set_title('Color')\n",
    "    ax2.imshow(lab_img[:,:,0], cmap = 'nipy_spectral')\n",
    "    ax2.set_title('Labels')\n",
    "    if lab_img.max()>0.1:\n",
    "        xd, yd = np.where(lab_img[:,:,0]>0)\n",
    "        bound_img = mark_boundaries(image = r_rgb_img, label_img = lab_img[:,:,0], \n",
    "                                    color = (1,0,0), background_label = 255, mode = 'thick')\n",
    "        ax3.imshow(bound_img[xd.min():xd.max(), yd.min():yd.max(),:])\n",
    "        ax3.set_title('Cropped Overlay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "cd584b6c-c185-4519-96b6-047fa587386b",
    "_uuid": "33b097924b25db4b5863e0abce591c03ccddae0a"
   },
   "outputs": [],
   "source": [
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, BatchNormalization, Dropout, Lambda\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "# Build U-Net model\n",
    "inputs = Input(IMG_SIZE+(3,))\n",
    "s = BatchNormalization()(inputs) # we can learn the normalization step\n",
    "s = Dropout(0.5)(s)\n",
    "\n",
    "c1 = Conv2D(8, (3, 3), activation='relu', padding='same') (s)\n",
    "c1 = Conv2D(8, (3, 3), activation='relu', padding='same') (c1)\n",
    "p1 = MaxPooling2D((2, 2)) (c1)\n",
    "\n",
    "c2 = Conv2D(16, (3, 3), activation='relu', padding='same') (p1)\n",
    "c2 = Conv2D(16, (3, 3), activation='relu', padding='same') (c2)\n",
    "p2 = MaxPooling2D((2, 2)) (c2)\n",
    "\n",
    "c3 = Conv2D(32, (3, 3), activation='relu', padding='same') (p2)\n",
    "c3 = Conv2D(32, (3, 3), activation='relu', padding='same') (c3)\n",
    "p3 = MaxPooling2D((2, 2)) (c3)\n",
    "\n",
    "c4 = Conv2D(64, (3, 3), activation='relu', padding='same') (p3)\n",
    "c4 = Conv2D(64, (3, 3), activation='relu', padding='same') (c4)\n",
    "p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
    "\n",
    "c5 = Conv2D(128, (3, 3), activation='relu', padding='same') (p4)\n",
    "c5 = Conv2D(128, (3, 3), activation='relu', padding='same') (c5)\n",
    "\n",
    "u6 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c5)\n",
    "u6 = concatenate([u6, c4])\n",
    "c6 = Conv2D(64, (3, 3), activation='relu', padding='same') (u6)\n",
    "c6 = Conv2D(64, (3, 3), activation='relu', padding='same') (c6)\n",
    "\n",
    "u7 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c6)\n",
    "u7 = concatenate([u7, c3])\n",
    "c7 = Conv2D(32, (3, 3), activation='relu', padding='same') (u7)\n",
    "c7 = Conv2D(32, (3, 3), activation='relu', padding='same') (c7)\n",
    "\n",
    "u8 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c7)\n",
    "u8 = concatenate([u8, c2])\n",
    "c8 = Conv2D(16, (3, 3), activation='relu', padding='same') (u8)\n",
    "c8 = Conv2D(16, (3, 3), activation='relu', padding='same') (c8)\n",
    "\n",
    "u9 = Conv2DTranspose(8, (2, 2), strides=(2, 2), padding='same') (c8)\n",
    "u9 = concatenate([u9, c1], axis=3)\n",
    "c9 = Conv2D(8, (3, 3), activation='relu', padding='same') (u9)\n",
    "c9 = Conv2D(8, (3, 3), activation='relu', padding='same') (c9)\n",
    "\n",
    "outputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)\n",
    "\n",
    "model = Model(inputs=[inputs], outputs=[outputs])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "1016dae1-b7a7-439e-9d5d-6ee661a4ff6e",
    "_uuid": "99c84b4c88ce8a6319bd5ed8830f1d9d864f5035",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "# Define IoU metric\n",
    "def mean_iou(y_true, y_pred):\n",
    "    prec = []\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        y_pred_ = tf.to_int32(y_pred > t)\n",
    "        score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([up_opt]):\n",
    "            score = tf.identity(score)\n",
    "        prec.append(score)\n",
    "    return K.mean(K.stack(prec))\n",
    "\n",
    "smooth = 1.\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer = 'adam', \n",
    "                   loss = dice_coef_loss, \n",
    "                   metrics = [dice_coef, 'binary_accuracy', 'mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fdf99eeb-1bbb-4e19-a106-238a38261edf",
    "_uuid": "4695ec7bd66e624aee98971a92c97268a5c68b0f"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "weight_path=\"{}_weights.best.hdf5\".format('unet')\n",
    "\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='loss', factor=0.8, patience=10, verbose=1, mode='auto', epsilon=0.0001, cooldown=5, min_lr=0.0001)\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=5) # probably needs to be more patient, but kaggle time is limited\n",
    "callbacks_list = [checkpoint, early, reduceLROnPlat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "775fc5c8-b93a-48fa-8711-368f7880985c",
    "_uuid": "631939949c70b3674b7b093b558fc0719cce0190"
   },
   "outputs": [],
   "source": [
    "# reset the generators so they all have different seeds when multiprocessing lets loose\n",
    "batch_size = 16\n",
    "train_and_lab_gen = train_and_lab_gen_func(train_split_df, batch_size = batch_size)\n",
    "valid_and_lab_gen = train_and_lab_gen_func(valid_split_df, batch_size = batch_size)\n",
    "model.fit_generator(train_and_lab_gen, \n",
    "                    steps_per_epoch = 2048//batch_size,\n",
    "                    validation_data = valid_and_lab_gen,\n",
    "                    validation_steps = 256//batch_size,\n",
    "                    epochs = 3, \n",
    "                    workers = 4,\n",
    "                    use_multiprocessing = True,\n",
    "                    callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7c4caaf8-3c31-4e0e-8298-8d9cc3f44c12",
    "_uuid": "b8e001a984e00b9aa70d9a418cb43e8913e4c8ea"
   },
   "source": [
    "# Showing the results\n",
    "Here we can preview the output of the model on a few examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "40f63c1e-4067-4702-878e-2f8c0474c401",
    "_uuid": "57800bbeb10c674020f31bc788518b89551626d9",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "(rgb_batch, lab_batch) = next(valid_and_lab_gen)\n",
    "sample_rows = 5\n",
    "fig, m_axs = plt.subplots(sample_rows, 5, figsize = (20, 6*sample_rows))\n",
    "[c_ax.axis('off') for c_ax in m_axs.flatten()]\n",
    "for (ax1, ax2, ax2_pred, ax3, ax3_pred), rgb_img, lab_img in zip(m_axs, rgb_batch, lab_batch):\n",
    "    # undoing the vgg correction is tedious\n",
    "    r_rgb_img = np.clip(rgb_img+110, 0, 255).astype(np.uint8)\n",
    "    lab_pred = model.predict(np.expand_dims(rgb_img, 0))[0]\n",
    "    \n",
    "    ax1.imshow(r_rgb_img)\n",
    "    ax1.set_title('Color')\n",
    "    ax2.imshow(lab_img[:,:,0], cmap = 'jet')\n",
    "    ax2.set_title('Labels')\n",
    "    ax2_pred.imshow(lab_pred[:,:,0], cmap = 'jet')\n",
    "    ax2_pred.set_title('Pred Labels')\n",
    "    if lab_img.max()>0.1:\n",
    "        xd, yd = np.where(lab_img[:,:,0]>0)\n",
    "        bound_img = mark_boundaries(image = r_rgb_img, label_img = lab_img[:,:,0], \n",
    "                                    color = (1,0,0), background_label = 255, mode = 'thick')\n",
    "        ax3.imshow(bound_img[xd.min():xd.max(), yd.min():yd.max(),:])\n",
    "        ax3.set_title('Cropped Overlay')\n",
    "        ax3_pred.imshow(lab_pred[xd.min():xd.max(), yd.min():yd.max(),0], cmap = 'jet')\n",
    "        ax3_pred.set_title('Cropped Overlay')\n",
    "fig.savefig('trained_model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b305f765-d263-4792-85f8-0192a8df2a35",
    "_uuid": "e49199a14b41cff1cc8ee7fc8908219be633ac9e",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
